{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get features (stops words removed) by tokenizing corpus - no stemming in baseline\n",
    "#Binary encoding\n",
    "#Assign target group \n",
    "#Use mutual information to get final feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData():\n",
    "    #Import Labelled Data\n",
    "    DATA_DIR = \"Data\"\n",
    "    thispath = Path().absolute()\n",
    "    #dtype = {\"index\": str, \"title\": str, \"description\": str, \"url\": str, \"date\": str, \"Retail Relevance\": str, \"Economy Relevant\": str, \"Market moving\": str}\n",
    "    RET_ARTICLES = os.path.join(DATA_DIR, \"retailarticles-18-11-06.xlsx\")\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(RET_ARTICLES)\n",
    "\n",
    "    try:\n",
    "        df.head()\n",
    "    except:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignStopWords(): \n",
    "    #Stop_words list Options\n",
    "    #Variation 1: added stop words starting at 'one'\n",
    "    stop_words = {'audio','i', 'me', 'us', 'my','myself','we','our','ours', 'ourselves','you', 'your', 'yours', 'yourself', 'yourselves','he',\t 'him',\t 'his',\t 'himself',\t 'she',\t 'her',\t 'hers',\t 'herself',\t 'it',\t 'its',\t 'itself',\t 'they','them','their', 'theirs', 'themselves', 'what', 'which', 'who','whom', 'this', 'that', 'these', 'those',\t 'am',\t 'is',\t 'are',\t 'was',\t 'were',\t 'be',\t 'been',\t 'being',\t 'have',\t 'has',\t 'had',\t 'having',\t 'do',\t 'does',\t 'did',\t 'doing',\t 'a',\t 'an',\t 'the',\t 'and',\t 'but',\t 'if',\t 'or',\t 'because',\t 'as',\t 'until',\t 'while',\t 'of',\t 'at',\t 'by',\t 'for',\t 'with',\t 'about',\t 'into',\t 'through',\t 'during',\t 'before',\t 'after',\t 'to',\t 'from','up','down','in','out','on','off','over',\t 'under',\t 'again',\t 'further',\t 'then',\t 'once',\t 'here',\t 'there',\t 'when',\t 'where',\t 'why',\t 'how',\t 'all',\t 'any',\t 'both',\t 'each',\t 'few',\t 'more',\t 'most',\t 'other',\t 'some',\t 'such',\t 'no',\t 'nor',\t 'not',\t 'only','own','same', 'so','than', 'too','very','s','t','can', 'will', 'just','don','should', 'now','one','two','twenty','three','thirty','four','forty','five','fifty','six','sixty','seven','seventy','eight','eighty','nine','ninety','ten','co','re','percent','make','example','would','18','says','put','includes','keep','already','continue','even','17','asked','enough','might','ve','8','amp','seems','ai','get','team','fox','side','give','tell','take','across','non','fact','0','looks','7','pace','monday','tuesday','wednesday','thursday','friday','saturday','use','30','11','read','programme','please','something','50','60','leave','using','car','musk', 'name','january','february','march','april','may','june','july','august','september','october','november','december'}\n",
    "\n",
    "    #from nltk.corpus import stopwords\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #print(stop_words)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_count_words(df, stop_words):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_counter = Counter()\n",
    "    for row in df.itertuples(index=True, name='Pandas'):\n",
    "            attribute = str((row, 'content'))\n",
    "            file_words = tokenizer.tokenize(attribute)\n",
    "            #keep lowercased words that are not stop words as features\n",
    "            file_wordsNS = [word.lower() for word in file_words if not word.lower() in stop_words]\n",
    "            # remove words that are numbers\n",
    "            file_wordsN = [word.lower() for word in file_wordsNS if not word.isnumeric()]\n",
    "            #remove words with a word length less than 4 (i.e. 1-3)\n",
    "            file_wordsF = [word.lower() for word in file_wordsN if not len(word)<4]\n",
    "            word_counter.update(file_wordsF)\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_cnt = corpus_count_words(df1,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_cnt.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary encoding for features, also appends retail target group\n",
    "def binary_encode_features(newsarticles, top_words):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df_rows = []\n",
    "    for row in newsarticles.itertuples(index=True, name='Pandas'):\n",
    "            attribute = str((row, 'content'))\n",
    "            file_words = tokenizer.tokenize(attribute)\n",
    "            df_rows.append([1 if word.lower() in file_words else 0 for word in top_words])      \n",
    "    X = pd.DataFrame(df_rows, columns = top_words)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInformation(B_Encoding, y, top_words): \n",
    "    #Estimate mutual information for a discrete target variable.\n",
    "    #Mutual information (MI) [1] between two random variables is a non-negative value, which measures the dependency between the variables.\n",
    "    #It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "    featureVals= mutual_info_classif(B_Encoding, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
    "    \n",
    "    np.asarray(featureVals)\n",
    "\n",
    "    Temp= pd.DataFrame(featureVals, columns = ['MI_Values'])\n",
    " \n",
    "    Final = Temp.assign(target_group = top_words)\n",
    "    \n",
    "    Highest_Features = Final.nlargest(250, 'MI_Values')\n",
    "    \n",
    "    return Highest_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeatures(**kwargs):\n",
    "    df = importData()\n",
    "    stop_words = assignStopWords()\n",
    "    \n",
    "    #Select subset of orig data\n",
    "    df1 = df[['content','Retail Relevance']]    \n",
    "    news_cnt = corpus_count_words(df1, stop_words)\n",
    "    \n",
    "    num_features = 1000\n",
    "    top_words = [word for (word, freq) in news_cnt.most_common(num_features)]\n",
    "    B_Encoding = binary_encode_features(df1, top_words)\n",
    "    y = df['Retail Relevance']\n",
    "    B_Encoding.assign(target_group=y)\n",
    "      \n",
    "    \n",
    "    Highest_Features = mutualInformation(B_Encoding, y, top_words)\n",
    "    Highest_Features = pd.DataFrame(Highest_Features)\n",
    "    \n",
    "    # Save as csv file in DATACOLLECTION data folder (bc it's needed for encoding script)\n",
    "    if ('csv' in kwargs) and (kwargs['csv']):\n",
    "        \n",
    "        # File path for this file\n",
    "        file_name = 'retailFeatureSet.csv'\n",
    "        thispath = Path().absolute()\n",
    "        OUTPUT_DIR = os.path.join(thispath, \"Data\", file_name)\n",
    "        \n",
    "        # if the following line throws an error, use the line after to save in same folder\n",
    "        pd.DataFrame.to_csv(Highest_Features, path_or_buf=OUTPUT_DIR)\n",
    "        #pd.DataFrame.to_csv(Highest_Features, path_or_buf=file_name)\n",
    "    \n",
    "    print(Highest_Features)\n",
    "    return Highest_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    HF = selectFeatures(csv = True)\n",
    "    return HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MI_Values target_group\n",
      "8     0.153042       target\n",
      "13    0.077407       stores\n",
      "15    0.074717       online\n",
      "37    0.074194     retailer\n",
      "171   0.067864        child\n",
      "44    0.067109    inflation\n",
      "48    0.064709     commerce\n",
      "26    0.057952         bank\n",
      "4     0.057738      company\n",
      "92    0.056130      central\n",
      "39    0.054358    retailers\n",
      "28    0.052232        store\n",
      "122   0.049500     shopping\n",
      "389   0.046995     monetary\n",
      "112   0.046033        rates\n",
      "193   0.044095     economic\n",
      "29    0.041950       retail\n",
      "68    0.041815       policy\n",
      "34    0.041051    customers\n",
      "124   0.040030      economy\n",
      "114   0.039981        chain\n",
      "59    0.038958     delivery\n",
      "85    0.038830   government\n",
      "583   0.038726   economists\n",
      "639   0.037134        bonds\n",
      "129   0.037038     interest\n",
      "111   0.036485        items\n",
      "76    0.034715     shoppers\n",
      "476   0.034528    officials\n",
      "159   0.034526       dollar\n",
      "..         ...          ...\n",
      "462   0.011315      fashion\n",
      "572   0.011307        wants\n",
      "513   0.011298         mark\n",
      "1     0.011217       amazon\n",
      "689   0.011138       crisis\n",
      "906   0.011123   deliveries\n",
      "796   0.011105    discounts\n",
      "816   0.011102        force\n",
      "649   0.011031        label\n",
      "355   0.011023     decision\n",
      "69    0.011004     services\n",
      "14    0.010921     business\n",
      "682   0.010871      numbers\n",
      "545   0.010798       rating\n",
      "225   0.010790         full\n",
      "526   0.010759        talks\n",
      "232   0.010726        power\n",
      "870   0.010716    valuation\n",
      "524   0.010676        labor\n",
      "303   0.010651        media\n",
      "862   0.010523     momentum\n",
      "538   0.010458        areas\n",
      "244   0.010457       nearly\n",
      "105   0.010383      against\n",
      "74    0.010365         home\n",
      "55    0.010356        prime\n",
      "905   0.010347      premium\n",
      "822   0.010321     adjusted\n",
      "556   0.010301     investor\n",
      "819   0.010290  marketplace\n",
      "\n",
      "[250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "Highest_Features = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pd.DataFrame(Highest_Features['target_group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSet = pd.DataFrame(Highest_Features['target_group'])\n",
    "    \n",
    "# Save as csv file in DATACOLLECTION data folder (bc it's needed for encoding script)\n",
    "\n",
    "\n",
    "# File path for this file\n",
    "file_name = 'retailFeatureSet.csv'\n",
    "thispath = Path().absolute()\n",
    "OUTPUT_DIR = os.path.join(thispath, \"Data\", file_name)\n",
    "\n",
    "# if the following line throws an error, use the line after to save in same folder\n",
    "pd.DataFrame.to_csv(featureSet, path_or_buf=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
