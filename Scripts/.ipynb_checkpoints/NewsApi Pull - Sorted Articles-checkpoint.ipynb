{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NEWSAPI_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-27712b9b7db5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#import API key (environment variable)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnewsapiKey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NEWSAPI_KEY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquerylist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfromdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtodate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;31m# raise KeyError with the original key value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'NEWSAPI_KEY'"
     ]
    }
   ],
   "source": [
    "# News API\n",
    "# This script pulls data from various news sources\n",
    "# Written by Jessie & Jade\n",
    "\n",
    "#import API key (environment variable)\n",
    "newsapiKey = os.environ['NEWSAPI_KEY']\n",
    "\n",
    "def News(querylist, sources, fromdate, todate):\n",
    "  \n",
    "    completequery = \"\"\n",
    "\n",
    "    #Create a string to query the url        \n",
    "    for i in range(len(querylist)): #defaults at index 0  \n",
    "        #Create a string to query the url     \n",
    "        if i < len(querylist)-1:\n",
    "            completequery += querylist[i]\n",
    "            completequery += \" OR \"\n",
    "        else:\n",
    "            completequery += querylist[i]\n",
    "        \n",
    "    print(completequery)\n",
    "    \n",
    "    \n",
    "        \n",
    "    #Find the first page\n",
    "    main_url = \" https://newsapi.org/v2/everything?q=(\" + completequery + \")&sources=\" + sources + \"&from=\" + fromdate + \"&to=\" + todate + \"&pageSize=100&page=1&apiKey=\" + newsapiKey  \n",
    "\n",
    "    \n",
    "    # fetching data in json format\n",
    "    open_bbc_page = requests.get(main_url).json() \n",
    "    totalResults = open_bbc_page[\"totalResults\"]\n",
    "    print(totalResults)\n",
    "    \n",
    "    #Write to CSV by page, until all articles in URL are written\n",
    "    j = 1\n",
    "    articlesToCSV(main_url, j) #print to csv at page 1 first\n",
    "    totalResults = totalResults - 100 \n",
    "    print(totalResults)\n",
    "    \n",
    "    while int(totalResults) > 0:\n",
    "        j = j + 1 #start printing to csv at page 2\n",
    "        main_url = \" https://newsapi.org/v2/everything?q=(\" + completequery + \")&sources=\" + sources + \"&from=\" + fromdate + \"&to=\" + todate + \"&pageSize=100&page=\" + str(j) + \"&apiKey=\" + newsapiKey  \n",
    "        articlesToCSV(main_url, j)\n",
    "        totalResults = totalResults - 100\n",
    "        print(totalResults)\n",
    "  \n",
    "    \n",
    "def articlesToCSV(main_url, k):\n",
    "    # getting all articles in a string article\n",
    "    open_bbc_page = requests.get(main_url).json()  \n",
    "    article = open_bbc_page[\"articles\"]\n",
    "    \n",
    "    # empty list which will contain all trending news\n",
    "    titles = []\n",
    "    description = []\n",
    "    url = []\n",
    "    publishedAt = []\n",
    "    content = []\n",
    "    \n",
    "    for ar in article:\n",
    "        titles.append(ar[\"title\"])\n",
    "        description.append(ar[\"description\"])\n",
    "        url.append(ar[\"url\"])\n",
    "        publishedAt.append(ar[\"publishedAt\"])\n",
    "        content.append(ar[\"content\"])                \n",
    "\n",
    "             # writing all trending news to csv        \n",
    "    with open('articles.csv', 'a', newline='',encoding='utf-8-sig') as f:\n",
    "        articlewriter = csv.writer(f, delimiter = ',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(titles)):\n",
    "            s = (((k-1)*100) + i + 1, titles[i], description[i], url[i], publishedAt[i], content[i])\n",
    "            articlewriter.writerow(s)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortarticles():\n",
    "    #If You would like the articles sorted as well, run this code before opening the articles.csv file \n",
    "\n",
    "    articles = pd.read_csv(\"articles.csv\", header= None)\n",
    "    articles.columns = [\"index\", \"title\", \"description\", \"url\", \"date\", \"content\"]\n",
    "    articles.head()\n",
    "    \n",
    "    # convert column to datetype\n",
    "    articles['date']=pd.to_datetime(articles.date)\n",
    "    \n",
    "    #Sort by date and export as xlsx (easier to work with as xlsx)\n",
    "    articles = articles.sort_values(by='date')\n",
    "    writer = pd.ExcelWriter('sortedarticles.xlsx')\n",
    "    articles.to_excel(writer,'Sheet1')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Green Plains) OR (CF Industries) OR (Miracle-Gro) OR (Miracle Gro) OR (Tyson Foods) OR (Dean Foods) OR Nutrien OR (Mosaic Company) OR (Archer-Daniels) OR Archer Daniels OR (Del Monte) OR (Calavo Growers)\n",
      "1060\n",
      "960\n",
      "860\n",
      "760\n",
      "660\n",
      "560\n",
      "460\n",
      "360\n",
      "260\n",
      "160\n",
      "60\n",
      "-40\n"
     ]
    }
   ],
   "source": [
    "# Driver Code\n",
    "if __name__ == '__main__': \n",
    "    # function call\n",
    "    #News APi can only take 20 queries\n",
    "    #oldquerylist = [\"Amazon\", \"Walmart\", \"Home Depot\", \"Comcast\", \"Disney\", \"Netflix\", \"McDonald's\", \"Costco\", \"Lowe's\", \"Twenty-First Century\", \"Century Fox\", \"Starbucks\", \"Charter Communications\", \"TJX\", \"American Tower\", \"Simon Property\", \"Las Vegas Sands\", \"Crown Castle\", \"Target\", \"Carnival\", \"Marriott\", \"Sherwin-Williams\", \"Prologis\"]\n",
    "    \n",
    "    #AgriCompaniesStocks = [\"GPRE\", \"CF\", \"SMG\", \"TSN\", \"DF\", \"NTR\", \"MOS\", \"ADM\", \"FDP\", \"CVGW\"]\n",
    "    AgriCompanies= [\"(Green Plains)\", \"(CF Industries)\", \"(Miracle-Gro)\", \"(Miracle Gro)\", \"(Tyson Foods)\", \"(Dean Foods)\", \"Nutrien\", \"(Mosaic Company)\", \"(Archer-Daniels)\",\"Archer Daniels\", \"(Del Monte)\", \"(Calavo Growers)\"]\n",
    "    SourcesPt1 = \"abc-news,al-jazeera-english,associated-press,australian-financial-review,axios,bbc-news,bloomberg,business-insider,cbc-news,cbs-news,cnbc,cnn,financial-post,financial-times,fortune,fox-news,google-news,google-news-ca,independent,msnbc,national-greographic\"\n",
    "    SourcesPt2 = \"national-review, nbc-news,newsweek,new-york-magazine,politico,recode,reuters,new-scientist,techcrunch,the-globe-and-mail,the-economist,the-huffinton-post,the-new-york-times,the-wall-street-journal,the-washington-post,time,usa-today,wired\"\n",
    "    BusinessSources = \"bloomberg,cnbc,fortune,financial-times,financial-post,the-economist,the-wall-street-journal\" #business-insider excluded. \n",
    "    \n",
    "    #Define Dates to Gather Data, can set manual dates or use Today -1\n",
    "    Pull_From = \"2017-10-30\"\n",
    "    Pull_To = \"2018-10-28\"\n",
    "    #Today's date \n",
    "    today = datetime.today().strftime('%Y-%m-%dT:%H:%M:00')\n",
    "    yesterday = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%dT:%H:%M:00')\n",
    "\n",
    "    \n",
    "    #Define Companies to query on, if more than one word, include brackets\n",
    "    RetailCompaniesStocks = [\"GPS\", \"FL\", \"LB\", \"MAC\", \"KIM\", \"TJX\", \"CVS\", \"HD\", \"BBY\", \"LOW\"]\n",
    "    RetailCompanies1 = [\"(Gap Inc)\", \"(Foot Locker)\", \"(L Brands)\", \"Macerich\", \"Kimco\", \"TJX\", \"CVS\", \"(Home Depot)\", \"(Best Buy)\", \"(Lowe's)\" ]\n",
    "    RetailCompanies2 = [\"Walmart\", \"Target\", \"Amazon\", \"Kroger\", \"Walgreens\", \"Target\", \"Kohl's\", \"(Dollar General)\", \"(Bed Bath and Beyond)\", \"Safeway\"]\n",
    "    \n",
    "    #Run to collect articles that fit within your query\n",
    "    News(AgriCompanies, BusinessSources, yesterday, today) \n",
    "\n",
    "    #Function call below to sort the articles by date\n",
    "    sortarticles()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
