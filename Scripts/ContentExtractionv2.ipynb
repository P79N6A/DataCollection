{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import pprint as pp\n",
    "\n",
    "# Data packages\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "#nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData():\n",
    "    #Import Labelled Data\n",
    "    DATA_DIR = \"Data\"\n",
    "    thispath = Path().absolute()\n",
    "    #dtype = {\"index\": str, \"title\": str, \"description\": str, \"url\": str, \"date\": str, \"Retail Relevance\": str, \"Economy Relevant\": str, \"Market moving\": str}\n",
    "    RET_ARTICLES = os.path.join(DATA_DIR, \"retailarticles-18-11-06.xlsx\")\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(RET_ARTICLES)\n",
    "\n",
    "    try:\n",
    "        df.head()\n",
    "    except:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Utility functions for filtering content\n",
    "    originally written by: vipul-sharma20\n",
    "    modifications made by: jadekhiev\n",
    "\"\"\"\n",
    "def getWords(sentence):\n",
    "    \"\"\"\n",
    "    Extracts words/tokens from a sentence\n",
    "    :param sentence: (str) sentence\n",
    "    :returns: list of tokens\n",
    "    \"\"\"\n",
    "    stopwords = [\n",
    "        # months\n",
    "        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"decemeber\",\n",
    "        # symbols that don't separate a sentence\n",
    "        '$','“','”','’','—',\n",
    "        # specific article terms that are useless\n",
    "        \"read\", \"share\", \"file\", \"'s\",\"i\", \"photo\", \"percent\",\"s\", \"t\", \"inc.\", \"corp\", \"group\", \"inc\", \"corp.\", \"source\", \"bloomberg\", \"CNBC\",\n",
    "        # useless pronouns\n",
    "        \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"co.\", \"inc.\",\n",
    "        # etc\n",
    "        \"the\", \"a\", \"of\", \"have\", \"has\", \"had\", \"having\"\n",
    "        #\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"while\", \"of\", \"at\", \"by\", \"for\", \"about\", \"into\", \"through\", \"during\", \"before\", \"after\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"just\", \"don\", \"now\"\n",
    "        ]\n",
    "    words = word_tokenize(sentence)\n",
    "    words = ([word for word in words if word.lower() not in stopwords])\n",
    "    #print(words)\n",
    "    return words\n",
    "\n",
    "def getParagraphs(content):\n",
    "    \"\"\"\n",
    "    Exctracts paragraphs from the the text content\n",
    "    :param content: (str) text content\n",
    "    :returns: list of paragraphs\n",
    "    \"\"\"\n",
    "    paraList = content.split('\\n\\n')\n",
    "    return paraList\n",
    "\n",
    "\n",
    "def getSentences(paragraph):\n",
    "    \"\"\"\n",
    "    Extracts sentences from a paragraph\n",
    "    :param paragraph: (str) paragraph text\n",
    "    :returns: list of sentences\n",
    "    \"\"\"\n",
    "    indexed = {}\n",
    "    sentenceList = tokenize.sent_tokenize(paragraph)\n",
    "    for i, s in enumerate(sentenceList):\n",
    "        indexed[i] = s\n",
    "    return sentenceList, indexed\n",
    "\n",
    "def countWords(wordList):\n",
    "    return dict(Counter(wordList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractContext(content):\n",
    "\"\"\"\n",
    "Script to extract important topics from content\n",
    "originally written by: vipul-sharma20\n",
    "modifications made by: jadekhiev\n",
    "\"\"\"\n",
    "    def __init__(self):\n",
    "        train = brown.tagged_sents(categories='news')\n",
    "\n",
    "        # backoff regex tagging\n",
    "        regex_tag = nltk.RegexpTagger([\n",
    "             #(r'[$][0-9]+\\s[MmBbTt]\\S+','DV'), #dollar value \n",
    "             (r'^[-\\:]?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "             (r'.*able$', 'JJ'),\n",
    "             (r'^[A-Z].*$', 'NNP'),\n",
    "             (r'.*ly$', 'RB'),\n",
    "             (r'.*s$', 'NNS'),\n",
    "             (r'.*ing$', 'VBG'),\n",
    "             (r'.*ed$', 'VBD'),\n",
    "             (r'.[\\/\\/]\\S+', 'URL'), #URL / useless\n",
    "             (r'.*', 'NN')\n",
    "        ])\n",
    "\n",
    "        unigram_tag = nltk.UnigramTagger(train, backoff=regex_tag)\n",
    "        bigram_tag = nltk.BigramTagger(train, backoff=unigram_tag)\n",
    "        trigram_tag = nltk.TrigramTagger(train, backoff=bigram_tag)\n",
    "\n",
    "        # custom defined Context Free Grammar (CFG) by vipul\n",
    "        cfg = dict()\n",
    "        cfg['NNP+NNP'] = 'NNP'\n",
    "        cfg['NN+NN'] = 'NNI'\n",
    "        cfg['NNP+NNI'] = 'NNI'\n",
    "        cfg['NNI+NN'] = 'NNI'\n",
    "        cfg['NNI+NNI'] = 'NNI'\n",
    "        cfg['NNI+NNP'] = 'NNI'\n",
    "        cfg['JJ+JJ'] = 'JJ'\n",
    "        cfg['JJ+NN'] = 'NNI'\n",
    "        cfg['CD+CD'] = 'CD'\n",
    "        cfg['NPI+NNP'] = 'NNP' # this is specific for collecting terms with the word deal\n",
    "        cfg['NNI+RP'] = 'NNI' # collects terms like \"heats up\"\n",
    "        cfg['RB+NN'] = 'NNP'# combination for monetary movement e.g. quarterly[RB] profit[NN] fell [VBD]\n",
    "        cfg['NNP+VBD'] = 'VPI' #VBP = a verb phrase\n",
    "        cfg['MD+VB'] = 'VPI' # collects terms like \"will lose\" (verb phrase incomplete)\n",
    "        cfg['MD+NN'] = 'VPI' # collects terms like \"will soar\" (verb phrase incomplete)\n",
    "        cfg['VPI+NN'] = 'VP' # collects terms like \"will lose ground\"\n",
    "        cfg['NNI+VP'] = 'VP' # collects terms like \"index will soar\"\n",
    "        cfg['NN+VPI'] = 'VP' # collects terms like \"index will soar\"\n",
    "        cfg['NNP+VPI'] = 'VP' # collects terms like \"index will soar\"\n",
    "        cfg['VPI+TO'] = 'VPI' # collect past participle verbs with to e.g. pledged to\n",
    "        cfg['VBN+TO'] = 'VBN' # collect past participle verbs with to e.g. pledged to\n",
    "        cfg['VBN+NN'] = 'VP' # collects terms like \"pledged to adapt\"\n",
    "\n",
    "    def get_info(self, content):\n",
    "        words = getWords(content)\n",
    "        temp_tags = trigram_tag.tag(words)\n",
    "        tags = re_tag(temp_tags)\n",
    "        normalized = True\n",
    "        while normalized:\n",
    "            normalized = False\n",
    "            #print(\"len tag: \", len(tags))\n",
    "            #pp.pprint(DictGroupBy(tags))\n",
    "            for i in range(0, len(tags) - 1):\n",
    "                #print(\"i: \", i)\n",
    "                tagged1 = tags[i]\n",
    "                if i+1 >= len(tags) - 1:\n",
    "                    break\n",
    "                tagged2 = tags[i+1]\n",
    "\n",
    "                # when word = deal and next word is tagged IN (with, for, etc.) \n",
    "                if tagged1[0]=='deal' and tagged2[1]=='IN':\n",
    "                    tags.pop(i)\n",
    "                    tags.pop(i)\n",
    "                    re_tagged = tagged1[0] + ' ' + tagged2[0]\n",
    "                    pos='NPI'\n",
    "                    tags.insert(i, (re_tagged, pos))\n",
    "                    normalized = True\n",
    "\n",
    "                else: \n",
    "                    key = tagged1[1] + '+' + tagged2[1]\n",
    "                    pos = cfg.get(key)       \n",
    "                    if pos:\n",
    "                        tags.pop(i)\n",
    "                        tags.pop(i)\n",
    "                        re_tagged = tagged1[0] + ' ' + tagged2[0]\n",
    "                        tags.insert(i, (re_tagged, pos))\n",
    "                        normalized = True\n",
    "\n",
    "        final_context = []\n",
    "        for tag in tags:\n",
    "            if tag[1] == 'NNP' or tag[1] == 'NNI' or tag[1] == 'VP':\n",
    "                final_context.append(tag[0])\n",
    "        return final_context\n",
    "\n",
    "\n",
    "    def re_tag(self, tagged):\n",
    "        new_tagged = []\n",
    "        for tag in tagged:\n",
    "            if tag[1] == 'NP' or tag[1] == 'NP-TL':\n",
    "                new_tagged.append((tag[0], 'NNP'))\n",
    "            elif tag[1][-3:] == '-TL':\n",
    "                new_tagged.append((tag[0], tag[1][:-3]))\n",
    "            elif tag[1][-1:] == 'S':\n",
    "                new_tagged.append((tag[0], tag[1][:-1]))\n",
    "            else:\n",
    "                new_tagged.append((tag[0], tag[1]))\n",
    "        return new_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleExtract():\n",
    "\"\"\"\n",
    "Sample context extractor for random article\n",
    "\"\"\"\n",
    "    def __init__(self):\n",
    "        # import articles\n",
    "        articleDf = importData()\n",
    "        # create new column with a cleaned up date so that it is possible to filter easily\n",
    "        articleDf['dateCleaned'] = pd.to_datetime(articleDf['date'].str[0:10])\n",
    "        # select random article\n",
    "        artNum = np.rand(0,len(articlesDf.index))\n",
    "        content = articleDf['content'].iloc[artNum]\n",
    "        context = ExtractContext(content)\n",
    "\n",
    "    def displayResults(self, context):\n",
    "        #context = [term for term in context]# if not (''in term ==True) and len(term.split()) > 1]\n",
    "        wordCount = countWords(context)\n",
    "        print(\"title: \" + articleDf['title'].iloc[artNum])\n",
    "        print(\"description: \" + articleDf['description'].iloc[artNum])\n",
    "        print(\"url: \" + articleDf['url'].iloc[artNum])\n",
    "        print(\"content: \" + content)\n",
    "        print(\"context:\")\n",
    "        print([term for term, count in wordCount.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateRangeExtract(startDate, endDate):\n",
    "\"\"\"\n",
    "Context extractor for specific date range\n",
    "\"\"\"\n",
    "    def __init__(self):\n",
    "        # import articles\n",
    "        articleDf = importData()\n",
    "        # create new column with a cleaned up date so that it is possible to filter easily\n",
    "        articleDf['dateCleaned'] = pd.to_datetime(articleDf['date'].str[0:10])\n",
    "        # extract date range\n",
    "        dateFilteredDf = articleDf[articleDf['dateCleaned'].isin(pd.date_range(startDate, endDate))]\n",
    "    \n",
    "    def contextExtract(self, dateFilteredDf):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
